{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webvtt-py\n",
      "  Downloading webvtt_py-0.4.5-py3-none-any.whl (16 kB)\n",
      "Processing /home/raltman/.cache/pip/wheels/9b/04/dd/7daf4150b6d9b12949298737de9431a324d4b797ffd63f526e/docopt-0.6.2-py2.py3-none-any.whl\n",
      "Installing collected packages: docopt, webvtt-py\n",
      "Successfully installed docopt-0.6.2 webvtt-py-0.4.5\n"
     ]
    }
   ],
   "source": [
    "!pip install --user webvtt-py tqdm inflect spacy && spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import webvtt as wv\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from typing import Iterator, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_on_pauses(vtt_parser, split_time: float = 2.) -> Iterator[Dict[str, Any]]:\n",
    "    name = Path(vtt_parser.file).name\n",
    "    duration = vtt_parser.total_length\n",
    "    start, end = 0, 0\n",
    "    text = \"\"\n",
    "    for i, caption in enumerate(vtt_parser):\n",
    "        if i % 2 == 1:  # Every other line is a new caption\n",
    "            if (caption.start_in_seconds - end > split_time) and text:  # Split on extended silence\n",
    "                yield dict(name=name, duration=duration, start=start, end=end, text=text)\n",
    "                start = caption.start_in_seconds\n",
    "                text = \"\"\n",
    "            line = re.sub('\\[.*\\]', '', caption.text)  # Remove [Music] or [Laughter] captions\n",
    "            line = re.sub('\\s+', ' ', line)  # Condense whitespace\n",
    "            if text:\n",
    "                text += ' '\n",
    "            text += line.strip()\n",
    "            end = caption.end_in_seconds\n",
    "        i += 1\n",
    "    if text:\n",
    "        yield dict(name=name, duration=duration, start=start, end=end, text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 655/655 [00:19<00:00, 34.45it/s]\n"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "files = list(Path('data').glob('*.en.vtt'))\n",
    "for file in tqdm(files):\n",
    "    vtt_parser = wv.read(file)\n",
    "    records.extend(split_on_pauses(vtt_parser))\n",
    "data = pd.DataFrame.from_records(records)\n",
    "data = data.dropna()  # Drop empty lines of text\n",
    "data = data.drop_duplicates(subset='text')  # Drop lines from duplicate videos\n",
    "data.to_csv(\"data/raw_dataset.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 99.79 hours of content\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/raw_dataset.tsv\", sep='\\t')\n",
    "content_duration = data.groupby('name').head(1).duration.sum() / (60 * 60)\n",
    "print(f\"Dataset contains {content_duration:.2f} hours of content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess for text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import inflect\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_to_words = inflect.engine().number_to_words\n",
    "all_numbers_to_words = lambda text: re.sub('([0-9]+[\\.,]*[0-9]*)', lambda x: number_to_words(x[1]), text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 7.39 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "data = pd.read_csv(\"data/raw_dataset.tsv\", sep='\\t')\n",
    "data.text = data.text.apply(all_numbers_to_words)  # Convert numbers to words\n",
    "data.text = data.text.str.replace('-', ' ')  # Remove hyphens\n",
    "data.to_csv(\"data/cleaned_dataset.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do train/valid/test split\n",
    "train_frac = 0.9\n",
    "valid_frac = 0.1\n",
    "random_seed = 42\n",
    "\n",
    "video_data = data.groupby('name').head(1)\n",
    "video_data = video_data.sample(frac=1, random_state=random_seed)\n",
    "video_names = video_data.name\n",
    "video_fracs = video_data.duration.cumsum() / video_data.duration.sum()\n",
    "\n",
    "with open(\"data/corpus.train.txt\", 'w') as fh:\n",
    "    train_names = video_names[(video_fracs < train_frac)]\n",
    "    for name in train_names:\n",
    "        train_lines = data[data.name == name].text\n",
    "        print(*train_lines, sep='. ', file=fh)\n",
    "with open(\"data/corpus.valid.txt\", 'w') as fh:\n",
    "    valid_names = video_names[(train_frac <= video_fracs) & (video_fracs < train_frac + valid_frac)]\n",
    "    for name in valid_names:\n",
    "        valid_lines = data[data.name == name].text\n",
    "        print(*valid_lines, sep='. ', file=fh)\n",
    "with open(\"data/corpus.test.txt\", 'w') as fh:\n",
    "    test_names = video_names[train_frac + valid_frac < video_fracs]\n",
    "    for name in test_names:\n",
    "        test_lines = data[data.name == name].text\n",
    "        print(*test_lines, sep='. ', file=fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/corpus.test.txt\n",
      "data/corpus.train.txt\n",
      "data/corpus.valid.txt\n",
      "{\"success\":true,\"key\":\"Vr3tfD7AVqo6\",\"link\":\"https://file.io/Vr3tfD7AVqo6\",\"expiry\":\"14 days\"}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1535k  100    94  100 1535k    661  10.5M --:--:-- --:--:-- --:--:-- 10.5M\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tar czfv data/corpus.tar.gz data/corpus.*.txt\n",
    "curl -F \"file=@data/corpus.tar.gz\" https://file.io/?expires=1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess for BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spacy.load('en', disable=['parser', 'ner'])\n",
    "lemmatize = lambda text: ' '.join([x.text if x.lemma_ == '-PRON-' else x.lemma_ for x in sp(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67098/67098 [03:05<00:00, 362.46it/s]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/cleaned_dataset.tsv\", sep='\\t')\n",
    "data.text = data.text.apply(str.lower)  # All words to lowercase\n",
    "data.text = data.text.str.replace(f'[{string.punctuation}]', '')  # Remove all common punctuation\n",
    "data.text = data.text.progress_apply(lemmatize)  # Lemmatize tokens\n",
    "data.to_csv(\"data/lemmatized_dataset.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
